### Title

Describe, Then Score: Language-Guided Image Complexity Assessment

描述，再打分：语言引导的图像复杂度评估

### 1. Introduction

图像复杂度是计算机视觉与多模态学习中一个基础但长期被低估的属性。它不仅影响数据集构建时样本的代表性与训练难度，还直接决定生成模型在高信息量场景中的表现是否可靠。在自动标注、主动学习、难例挖掘等应用中，复杂度评估更是筛选数据、分配计算资源的重要依据。因此，一个可解释、可计算、并能在不同场景下稳定适用的复杂度度量，不仅是分析任务的需要，也是构建健壮 AI 系统的基础能力。

在人工标注实践中，人们在判断一张图像的复杂度时，往往会**同时关注局部与整体**：既会观察细节区域的纹理、颜色、结构等低级视觉特征，也会综合考量图像中物体的数量、类别、空间关系、形态变化等高级语义信息。低级特征反映了像素层面的信息密度与变化模式，而高级语义则体现了场景的构成多样性与理解难度。这种多层次、多角度的评估方式保证了人类对复杂度的判断既细致又全面。

然而，现有方法大多只覆盖了其中一个维度。一类方法依赖视觉特征，例如边缘密度、对象数量、分形维数或视觉信息熵，能够捕捉低级纹理与结构多样性，但难以识别语义层面的复杂关系；另一类方法基于文本描述的语义特征，例如 caption 长度或语言模型困惑度，能够反映概念和事件的多样性，却无法刻画图像的细粒度视觉变化。单一模态带来了明显的“模态盲区”：纯视觉方法无法识别“物体简单但故事复杂”的情形，而纯语义方法则无法感知“语义单一但纹理极丰富”的场景。此外，这些方法多数依赖经验性特征组合，缺乏严谨的数学定义和可验证的理论分析，泛化能力也难以保证。

针对这些问题，我们提出 **Describe, Then Score** 框架，首次给出结合视觉与语义信息的统一复杂度定义。该定义将视觉熵与语义熵加权结合，分别刻画低级视觉多样性与高级语义多样性；其中语义部分由先进的视觉语言模型 Florence-2 生成，以保证描述的细节与准确性。在理论上，我们基于多模态信息瓶颈（MMIB）推导出复杂度预测的学习目标，并证明在语义与视觉存在互补信息时，多模态输入的泛化上界严格优于单模态。在实验上，我们不仅在主任务数据集上验证方法有效性，还将其迁移到与复杂度预测相似的任务场景，展示了定义与框架的跨域稳定性与可扩展性。

**本文的主要贡献如下：**

1. **统一的复杂度定义** —— 提出结合视觉熵与语义熵的图像复杂度度量，能够同时刻画低级视觉多样性（纹理、颜色、结构等）与高级语义多样性（物体数量、类别、空间关系等）。该定义契合人工在评估图像复杂度时同时关注局部与整体、低级与高级信息的认知规律。
2. **基于多模态信息瓶颈的预测框架** —— 构建基于多模态信息瓶颈（MMIB）的复杂度预测模型，并从理论上证明在视觉与语义信息互补的条件下，多模态输入的泛化误差上界严格优于单模态输入。
3. **充分的实验与迁移验证** —— 在目标任务数据集上验证所提方法的有效性，并将其迁移到与复杂度评估相关的相似任务场景，展示该定义与框架的跨域稳定性，以及与理论分析结果的一致性。

### 2. Related Work

#### 图像复杂度度量

早期的图像复杂度研究主要依赖低级视觉特征，如边缘密度、分割后的对象数量、分形维数、频谱能量分布等。这些方法能够捕捉像素或局部结构的变化，但对高级语义信息几乎不敏感。近年来，一些研究引入视觉信息熵（visual information entropy）等统计量来量化图像的低级多样性。然而，这类方法在面对“视觉简单但语义复杂”的场景时容易低估复杂度，缺乏对全局语义关系的刻画能力。

#### 文本复杂度与图像描述

在自然语言处理领域，文本复杂度度量常用可读性指数（如 Flesch-Kincaid）、语言模型困惑度（perplexity）、词汇多样性等指标。这些指标在图像描述（image captioning）的分析中被用于衡量生成文本的多样性与信息量。部分工作尝试将文本复杂度作为图像复杂度的间接估计，但由于依赖人工描述或自动生成的 caption，且未与视觉信息结合，因而在缺乏低级视觉特征时会出现失真。

#### 多模态学习与视觉-语言表示

多模态学习的发展（如 CLIP、BLIP、ALBEF、LXMERT 等）显著提升了视觉与语言信息对齐的能力。多模态特征融合方法包括简单拼接、注意力机制、跨模态对比学习等。这些方法主要针对分类、检索、生成等任务，较少直接应用于复杂度评估任务，也未在理论上刻画多模态融合对泛化能力的影响。

#### 信息瓶颈理论在多模态中的应用

信息瓶颈（Information Bottleneck, IB）为特征提取和泛化分析提供了统一的理论框架。在多模态任务中，IB 被用于互信息估计、冗余抑制、跨模态特征压缩等。然而，现有工作通常聚焦于任务性能优化，而非针对“复杂度预测”这种回归型任务建立理论分析，也未将 IB 与复杂度的数学定义结合起来。



### 3. Problem Formulation and Theoretical Analysis

#### 3.1 任务定义

给定一个输入图像 $I$，其来自于数据集$\mathcal{D}$，真实复杂度标签为 $y \in (0,1)$。利用任意caption生成模型 $g_{\phi}$ 从 $I$ 生成 caption $S = g_{\phi}(I)$，然后我们用多模态编码器 $f_{\theta}$ 预测复杂度：
$$
\hat{y} = f_{\theta}(I, S)
$$

学习目标是最小化预测误差：

$$
\min_{\theta} \ \mathbb{E}_{(I,y) \sim \mathcal{D}} \left[ \ell(f_{\theta}(I, S), y) \right]
$$

其中 $\ell$ 可为 MSE，$\theta$为模型参数。



#### 3.2 数学定义

首先在直觉上，复杂度应该随**视觉多样性**和**语义多样性**共同增加。我们将两者分别刻画为视觉熵$H_{\text{v}}(I)$和语义熵$H_{\text{s}}(S)$：

$$
H_{\text{v}}(I) = - \sum_{k=1}^K p_k^{(v)} \log p_k^{(v)}
$$

$$
H_{\text{s}}(S) = - \sum_{t=1}^T p_t^{(s)} \log p_t^{(s)}
$$

其中 $p_k^{(v)}$ 是视觉编码器对 patch 特征的 softmax 概率分布，$p_t^{(s)}$ 是文本编码器对 caption token 的 softmax 概率分布。进一步将二者组合得到图像复杂度：
$$
C(I) = \alpha\, H_{\mathrm{v}}(I) + \beta\, H_{\mathrm{s}}(S), \quad \alpha,\beta > 0
$$

其中， $\alpha,\beta$分别是视觉和语言融合的权重，可通过数据拟合确定。 $H_{\mathrm{v}}(I)$ 衡量图像在低级视觉空间的多样性（纹理、颜色、结构），$H_{\mathrm{s}}(S)$ 表示语言信息熵，衡量图像在语义空间的多样性（对象种类、关系、动作）。



#### 3.3 理论保证：信息瓶颈视角

我们首先形式化多模态融合下的信息表示学习问题。

**定义 1 （多模态表示）**
设视觉特征 $X_v = \phi_v(I)$，语义特征 $X_s = \phi_s(S)$，复杂度标签为 $Y$。我们希望学习一个表示 $T = h(X_v, X_s)$ 来预测 $Y$。融合表示 $T$ 应同时包含与 $Y$ 相关的信息并压缩冗余特征。根据 信息瓶颈 (Information Bottleneck, IB) 原理，最优表示应满足：
$$
T^* = \arg\max_T \ I(T; Y) - \lambda I(T; X_v, X_s)
$$

其中第一项用于维持表示中与复杂度相关的信息，第二项用于惩罚冗余与噪声信息。

**引理 1**
若 caption $S$ 与复杂度 $Y$ 存在正相关性，即 $I(X_s; Y) > 0$，则多模态融合表示相较于单视觉输入，至少不会降低互信息：
$$
I([X_v, X_s]; Y) \ge I(X_v; Y)
$$

**证明**
由互信息链式法则：
$$
I([X_v, X_s]; Y) = I(X_v; Y) + I(X_s; Y \mid X_v)
$$

由于条件互信息 $I(X_s; Y \mid X_v) \ge 0$，因此结论成立。

**推论 1**
在 IB 框架下，引入 caption 至少不会降低最优表示与复杂度的相关性。因此，多模态表示在理论上优于单模态表示。



#### 3.4 泛化保证：复杂度控制视角

我们进一步分析融合后的表示对模型泛化能力的影响。

**定义 2 （经验 Rademacher 复杂度）**
设假设空间$\mathcal{F}$，样本数 $n$，特征范数有界 $| \phi(I, S) | \le B$，特征维度为 $d$，则经验 Rademacher 复杂度满足：
$$
\hat{\mathcal{R}}_S(\mathcal{F}) \le \frac{B \sqrt{d}}{\sqrt{n}}
$$

**引理 2**
若语义特征 $X_s$ 对视觉特征 $X_v$ 起到压缩作用，使得有效维度 $d' < d$，则有：
$$
\hat{\mathcal{R}}'_S(\mathcal{F}) \le \frac{B \sqrt{d'}}{\sqrt{n}} < \frac{B \sqrt{d}}{\sqrt{n}}
$$

**定理 1 （泛化性提升）**
在视觉–语义融合框架下，若 caption 引入的信息对视觉特征空间起到压缩或正则化作用，则假设空间复杂度降低，从而提升模型的泛化能力。





### 4. Method

#### 4.1 Caption Generation

florence-2升成caption

#### 4.2 Feature Extraction

* ViT或CNN 提取视觉特征 → $H_{\text{vis}}$
* Transformer 提取文本 token 特征 → $H_{\text{sem}}$

#### 4.3 Cross Attention Fusion

* Cross-Attention 融合 $X_v, X_s$
* 最终预测复杂度分数

#### 4.4 Entropy Alignment Loss

<img src="C:\Users\liushipeng\AppData\Roaming\Typora\typora-user-images\image-20250816145203726.png" alt="image-20250816145203726" style="zoom: 15%;" />

在 3.2 中，我们分别定义了视觉熵 $H_v(I)$ 与语义熵 $H_s(S)$ 来刻画图像的复杂度。然而...........，若两者分布差异较大，融合后的表示会存在冗余，导致信息瓶颈约束下的 $I(T;X_v,X_s)$ 偏大。根据 3.3 信息瓶颈原理与 3.4 泛化界推导，这种冗余会降低特征压缩效率与泛化能力。因此，我们引入一个辅助任务：**约束 $H_v(I)$ 和 $H_s(S)$ 的分布接近**，以提升多模态协同。

我们记训练集上的视觉熵与语义熵集合分别为：

$$
\mathcal{H}_v = \{ H_v(I_i) \}_{i=1}^n, \quad \mathcal{H}_s = \{ H_s(S_i) \}_{i=1}^n
$$

由于 $\mathcal{H}_v, \mathcal{H}_s$ 都是一维实值随机变量的集合，因此使用 1D Wasserstein 距离 (Earth Mover’s Distance, EMD) 来度量两者的分布差异：

$$
\mathcal{L}_{\text{align}} = W_1(\mathcal{H}_v, \mathcal{H}_s)
$$

其中，1D Wasserstein 距离可简化为排序后数值的 $\ell_1$ 距离：

$$
W_1(\mathcal{H}_v, \mathcal{H}_s) = \frac{1}{n} \sum_{i=1}^n \left| H_v^{(i)} - H_s^{(i)} \right|
$$

这里 $H_v^{(i)}, H_s^{(i)}$ 分别表示对 $\mathcal{H}_v, \mathcal{H}_s$ 排序后的第 $i$ 个元素。

---

### 理论关联

1. **信息瓶颈 (IB)**

   * 对齐 \$H\_v\$ 和 \$H\_s\$ 分布可减少模态间冗余，使得表示 \$T\$ 在压缩 \$I(T; X\_v, X\_s)\$ 时更高效。
   * 即 \$\mathcal{L}\_{\text{align}}\$ 实际上是对 IB 的一个辅助实现。

2. **泛化界 (Rademacher Complexity)**

   * 当 \$H\_v\$ 与 \$H\_s\$ 分布更接近时，有效特征维度 \$d'\$ 更低（见公式 (4)）。
   * 从而降低 \$\hat{\mathcal{R}}'\_S(\mathcal{F})\$，提升模型的跨数据集泛化能力。

---

### 总体损失

结合主任务与辅助任务，最终优化目标为：

$$
\mathcal{L} = \mathcal{L}_{\text{task}} + \lambda_{\text{IB}} \, \mathcal{L}_{\text{IB}} + \lambda_{\text{align}} \, \mathcal{L}_{\text{align}}
$$

其中：

* \$\mathcal{L}\_{\text{task}} =\$ MSE + SRCC（监督复杂度预测）
* \$\mathcal{L}\_{\text{IB}} =\$ 信息瓶颈正则项（理论 3.3 对应）
* \$\mathcal{L}\_{\text{align}} =\$ 1D Wasserstein 熵分布对齐损失（理论 3.4 对应）

---

这样写法有两个好处：

* **方法和理论强绑定**：每个损失项都能回溯到上一章的公式和推导；
* **Wasserstein 的解释直观**：它衡量的是“最小搬运代价”，比 KL 更稳定（即使分布支持集不重叠也能计算）。





### 6. Experiments

1. **Datasets**：

   * IC9600（训练）
   * （测试迁移能力）
2. **Baselines**：

   * 单视觉方法（图像熵、ResNet回归）
   * 单语义方法（caption长度、语言模型困惑度）
   * 多模态融合（CLIP特征回归）
3. **Metrics**：

   * Pearson / Spearman 相关系数
   * RMSE / RMAE
4. **Results**：
   * 多模态显著优于单模态
   * 理论预测的趋势（互信息提升、泛化误差下降）在实验中得到验证

---

### 7. Ablation Study

* 去掉 $H_{\text{sem}}$ 或 $H_{\text{vis}}$ 的影响
* 不同 caption 质量的影响
* 信息瓶颈正则强度 $\lambda$ 对性能的作用曲线

---

### 8. Conclusion

* 提出**视觉+语义熵定义的图像复杂度**
* 基于**多模态信息瓶颈**推导理论框架
* 给出泛化界证明，验证理论 → 实验的一致性
* 为复杂度评估任务提供可扩展的理论基础
